The use of large language models (LLMs)
to solve mathematical problems is nowadays a heated topic. However, benchmarks
like GSM8k often involve problems solvable
with shallow heuristics and limited knowledge
scope, restricting the assessment of modelsâ€™
understanding of complex mathematical logic.
Various methods have been employed to enhance mathematical performance, each with
complementary strengths. To address these limitations, we investigated the performance of
multiple language models on the more challenging CONIC10K dataset and improved model
accuracy through code and program prompts
and fine-tuning. Additionally, we developed
an Automated Verification System using Regular Expressions and SymPy to ensure accurate
performance measurement. We think our work
is inspiring, the topic of combining code with
large language models still has strong development potential.
